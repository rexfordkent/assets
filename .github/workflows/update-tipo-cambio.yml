name: Actualizar Tipo de Cambio

on:
  schedule:
    - cron: '0 * * * *'
  workflow_dispatch:
  push:
    branches: [ main ]

permissions:
  contents: write

jobs:
  update-rates:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        pip install requests beautifulsoup4
    
    - name: Obtener tipo de cambio con ScraperAPI
      run: |
        cat > scraper.py << 'ENDOFSCRIPT'
        import requests
        from bs4 import BeautifulSoup
        import json
        import re
        from datetime import datetime
        import urllib3
        urllib3.disable_warnings()
        
        def get_sunat_scraperapi():
            """Obtener SUNAT usando ScraperAPI - VERSION MEJORADA"""
            try:
                print("Obteniendo SUNAT via ScraperAPI...")
                
                api_key = "a1057c3f6bbe27d6b7c97efa9f6f6f0a"
                target_url = "https://e-consulta.sunat.gob.pe/cl-at-ittipcam/tcS01Alias"
                
                # ScraperAPI endpoint
                scraper_url = f"http://api.scraperapi.com?api_key={api_key}&url={target_url}"
                
                print(f"Consultando ScraperAPI...")
                response = requests.get(scraper_url, timeout=30)
                
                print(f"Status: {response.status_code}")
                print(f"Content length: {len(response.text)}")
                
                if response.status_code == 200 and len(response.text) > 1000:
                    html = response.text
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # Buscar el dia actual
                    dia = datetime.now().day
                    print(f"Buscando dia {dia} en el calendario...")
                    
                    # Metodo 1: Buscar en tabla con estructura calendario
                    all_tds = soup.find_all('td')
                    
                    for i, td in enumerate(all_tds):
                        td_text = td.get_text(strip=True)
                        
                        # Si encontramos el dia
                        if td_text == str(dia):
                            print(f"Dia {dia} encontrado!")
                            
                            # Buscar valores en las siguientes celdas
                            for j in range(i+1, min(i+5, len(all_tds))):
                                next_td = all_tds[j]
                                next_text = next_td.get_text()
                                
                                # Buscar Compra y Venta
                                if 'Compra' in next_text and 'Venta' in next_text:
                                    numeros = re.findall(r'([0-9]+\.[0-9]+)', next_text)
                                    
                                    if len(numeros) >= 2:
                                        compra = float(numeros[0])
                                        venta = float(numeros[1])
                                        
                                        if 3.4 <= compra <= 3.8 and 3.4 <= venta <= 3.8:
                                            print(f"SUNAT encontrado: Compra={compra}, Venta={venta}")
                                            return {
                                                "compra": compra,
                                                "venta": venta,
                                                "fecha": f"{dia:02d}/{datetime.now().month:02d}/{datetime.now().year}",
                                                "fuente": "scraperapi"
                                            }
                    
                    # Metodo 2: Buscar cualquier valor 3.XXX en el HTML
                    print("Buscando valores por patron...")
                    valores = re.findall(r'3\.\d{3}', html)
                    
                    if valores:
                        print(f"Valores encontrados: {valores[:10]}")
                        valores_float = [float(v) for v in valores if 3.4 <= float(v) <= 3.8]
                        valores_unicos = list(dict.fromkeys(valores_float))
                        
                        if valores_unicos:
                            if len(valores_unicos) >= 2:
                                compra = min(valores_unicos[0], valores_unicos[1])
                                venta = max(valores_unicos[0], valores_unicos[1])
                            else:
                                compra = venta = valores_unicos[0]
                            
                            print(f"SUNAT por patron: {compra}/{venta}")
                            return {
                                "compra": compra,
                                "venta": venta,
                                "fecha": datetime.now().strftime("%d/%m/%Y"),
                                "fuente": "scraperapi_pattern"
                            }
                    
                    print("No se encontraron valores validos")
                else:
                    print(f"Error: Status {response.status_code}")
                    
            except Exception as e:
                print(f"Error con ScraperAPI: {e}")
                import traceback
                traceback.print_exc()
            
            return {"error": True, "message": "No se pudo obtener de SUNAT"}
        
        def get_sbs_scraperapi():
            """Obtener SBS - MANTIENE LA VERSION QUE FUNCIONA BIEN"""
            try:
                print("\nObteniendo SBS...")
                
                # PRIMERO intentar directo sin ScraperAPI (más rápido)
                response = requests.get(
                    'https://www.sbs.gob.pe/app/pp/sistip_portal/paginas/publicacion/tipocambiopromedio.aspx',
                    headers={'User-Agent': 'Mozilla/5.0'},
                    verify=False,
                    timeout=15
                )
                
                if response.status_code == 200:
                    html = response.text
                    
                    # Buscar valores 3.XXX en el HTML
                    valores = re.findall(r'3\.\d{3}', html)
                    
                    if valores:
                        valores_float = [float(v) for v in valores if 3.4 <= float(v) <= 3.8]
                        valores_unicos = list(dict.fromkeys(valores_float))
                        
                        if len(valores_unicos) >= 2:
                            print(f"SBS directo: {valores_unicos[0]}/{valores_unicos[1]}")
                            return {
                                "compra": valores_unicos[0],
                                "venta": valores_unicos[1],
                                "fecha": datetime.now().strftime("%d/%m/%Y"),
                                "fuente": "direct"
                            }
                        elif len(valores_unicos) == 1:
                            print(f"SBS un valor: {valores_unicos[0]}")
                            return {
                                "compra": valores_unicos[0],
                                "venta": valores_unicos[0],
                                "fecha": datetime.now().strftime("%d/%m/%Y"),
                                "fuente": "direct"
                            }
                    
                    # Si no encuentra con el método directo, intentar con ScraperAPI
                    print("Intentando SBS con ScraperAPI para mejor parsing...")
                    
                    api_key = "a1057c3f6bbe27d6b7c97efa9f6f6f0a"
                    target_url = "https://www.sbs.gob.pe/app/pp/sistip_portal/paginas/publicacion/tipocambiopromedio.aspx"
                    scraper_url = f"http://api.scraperapi.com?api_key={api_key}&url={target_url}"
                    
                    response2 = requests.get(scraper_url, timeout=30)
                    
                    if response2.status_code == 200:
                        html2 = response2.text
                        soup = BeautifulSoup(html2, 'html.parser')
                        
                        # Buscar tabla del dólar
                        tables = soup.find_all('table')
                        
                        for table in tables:
                            rows = table.find_all('tr')
                            for row in rows:
                                celdas = row.find_all('td')
                                if len(celdas) >= 3:
                                    texto = celdas[0].get_text(strip=True)
                                    
                                    if 'lar de N.A.' in texto or 'Dólar de N.A.' in texto:
                                        try:
                                            compra = float(celdas[1].get_text(strip=True))
                                            venta = float(celdas[2].get_text(strip=True))
                                            
                                            if 3.4 <= compra <= 3.8 and 3.4 <= venta <= 3.8:
                                                print(f"SBS via ScraperAPI tabla: {compra}/{venta}")
                                                return {
                                                    "compra": compra,
                                                    "venta": venta,
                                                    "fecha": datetime.now().strftime("%d/%m/%Y"),
                                                    "fuente": "scraperapi"
                                                }
                                        except:
                                            pass
                        
                        # Buscar por patron en ScraperAPI
                        valores = re.findall(r'3\.\d{3}', html2)
                        if valores:
                            valores_float = [float(v) for v in valores if 3.4 <= float(v) <= 3.8]
                            valores_unicos = list(dict.fromkeys(valores_float))
                            
                            if valores_unicos:
                                if len(valores_unicos) >= 2:
                                    return {
                                        "compra": valores_unicos[0],
                                        "venta": valores_unicos[1],
                                        "fecha": datetime.now().strftime("%d/%m/%Y"),
                                        "fuente": "scraperapi_pattern"
                                    }
                                else:
                                    return {
                                        "compra": valores_unicos[0],
                                        "venta": valores_unicos[0],
                                        "fecha": datetime.now().strftime("%d/%m/%Y"),
                                        "fuente": "scraperapi_pattern"
                                    }
                    
            except Exception as e:
                print(f"Error SBS: {e}")
            
            return {"error": True, "message": "No se pudo obtener de SBS"}
        
        # Ejecutar
        print("=" * 60)
        print("OBTENIENDO TIPO DE CAMBIO CON SCRAPERAPI")
        print("=" * 60)
        
        sunat_data = get_sunat_scraperapi()
        sbs_data = get_sbs_scraperapi()
        
        # Si SUNAT falla, usar valores de SBS
        if sunat_data.get('error') and not sbs_data.get('error'):
            print("\nSUNAT fallo, usando valores de SBS para ambos")
            sunat_data = {
                "compra": sbs_data['compra'],
                "venta": sbs_data['venta'],
                "fecha": sbs_data['fecha'],
                "fuente": "sbs_mirror"
            }
        
        result = {
            "sunat": sunat_data,
            "sbs": sbs_data,
            "ultima_actualizacion": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        with open('tipo-cambio.json', 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        print("\n" + "=" * 60)
        print("RESULTADO FINAL:")
        print(f"SUNAT: {sunat_data}")
        print(f"SBS: {sbs_data}")
        print("=" * 60)
        
        # Mostrar creditos restantes de ScraperAPI
        try:
            account_url = f"http://api.scraperapi.com/account?api_key=a1057c3f6bbe27d6b7c97efa9f6f6f0a"
            account_response = requests.get(account_url, timeout=5)
            if account_response.status_code == 200:
                account_data = account_response.json()
                print(f"\nScraperAPI - Requests restantes: {account_data.get('requestCount', 'N/A')}/5000")
        except:
            pass
        ENDOFSCRIPT
        
        python scraper.py
    
    - name: Mostrar JSON
      run: cat tipo-cambio.json
    
    - name: Commit
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add tipo-cambio.json
        git commit -m "TC via ScraperAPI - $(date +'%d/%m/%Y %H:%M')" || echo "No changes"
        git push || echo "Nothing to push"
