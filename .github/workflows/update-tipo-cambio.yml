name: Actualizar Tipo de Cambio

on:
  schedule:
    - cron: '0 * * * *'
  workflow_dispatch:
  push:
    branches: [ main ]

permissions:
  contents: write

jobs:
  update-rates:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        pip install requests beautifulsoup4
    
    - name: Obtener tipo de cambio con ScraperAPI
      run: |
        cat > scraper.py << 'ENDOFSCRIPT'
        import requests
        from bs4 import BeautifulSoup
        import json
        import re
        from datetime import datetime, timedelta
        import urllib3
        urllib3.disable_warnings()
        
        def get_sunat_scraperapi():
            """Obtener SUNAT - Simplemente usar SBS que ya funciona"""
            try:
                print("Obteniendo SUNAT (usando valores de SBS)...")
                
                # Como SUNAT = SBS del día anterior, y SBS ya funciona perfectamente,
                # simplemente usamos los mismos valores de SBS para SUNAT
                
                sbs_result = get_sbs_scraperapi()
                
                if not sbs_result.get('error'):
                    print(f"SUNAT usando valores de SBS: {sbs_result['compra']}/{sbs_result['venta']}")
                    return {
                        "compra": sbs_result['compra'],
                        "venta": sbs_result['venta'],
                        "fecha": datetime.now().strftime("%d/%m/%Y"),
                        "fuente": "sbs_data"
                    }
                    
            except Exception as e:
                print(f"Error: {e}")
            
            return {"error": True, "message": "No se pudo obtener de SUNAT"}
        
        def get_sbs_scraperapi():
            """Obtener SBS - Solo método directo sin ScraperAPI"""
            try:
                print("\nObteniendo SBS...")

                # Solo método directo, sin ScraperAPI
                response = requests.get(
                    'https://www.sbs.gob.pe/app/pp/sistip_portal/paginas/publicacion/tipocambiopromedio.aspx',
                    headers={'User-Agent': 'Mozilla/5.0'},
                    verify=False,
                    timeout=15
                )

                if response.status_code == 200:
                    html = response.text
                    soup = BeautifulSoup(html, 'html.parser')

                    # Extraer la fecha directamente del HTML
                    fecha_sbs = None
                    fecha_span = soup.find('span', {'id': lambda x: x and 'lblFecha' in x})
                    if fecha_span:
                        fecha_texto = fecha_span.get_text(strip=True)
                        # Extraer fecha del formato "Tipo de Cambio al 07/11/2025"
                        fecha_match = re.search(r'(\d{2}/\d{2}/\d{4})', fecha_texto)
                        if fecha_match:
                            fecha_sbs = fecha_match.group(1)
                            print(f"Fecha extraída del HTML: {fecha_sbs}")

                    # Buscar la tabla específica del dólar
                    tables = soup.find_all('table')

                    for table in tables:
                        rows = table.find_all('tr')
                        for row in rows:
                            celdas = row.find_all('td')
                            if len(celdas) >= 3:
                                texto = celdas[0].get_text(strip=True)

                                # Buscar específicamente "Dólar de N.A."
                                if 'lar de N.A.' in texto or 'Dólar de N.A.' in texto:
                                    try:
                                        compra_text = celdas[1].get_text(strip=True)
                                        venta_text = celdas[2].get_text(strip=True)

                                        compra = float(compra_text)
                                        venta = float(venta_text)

                                        if 3.0 <= compra <= 4.0 and 3.0 <= venta <= 4.0:
                                            # Si no se pudo extraer la fecha del HTML, calcularla
                                            if not fecha_sbs:
                                                fecha_hoy = datetime.now()
                                                dia_semana = fecha_hoy.weekday()  # 0=Lunes, 6=Domingo

                                                if dia_semana == 5:  # Sábado
                                                    fecha_calc = fecha_hoy - timedelta(days=1)  # Viernes
                                                elif dia_semana == 6:  # Domingo
                                                    fecha_calc = fecha_hoy - timedelta(days=2)  # Viernes
                                                else:
                                                    # Día de semana: SBS publica del día anterior
                                                    fecha_calc = fecha_hoy - timedelta(days=1)

                                                fecha_sbs = fecha_calc.strftime("%d/%m/%Y")

                                            print(f"SBS tabla: Compra={compra}, Venta={venta}, Fecha={fecha_sbs}")
                                            return {
                                                "compra": compra,
                                                "venta": venta,
                                                "fecha": fecha_sbs,
                                                "fuente": "direct"
                                            }
                                    except Exception as e:
                                        print(f"Error parsing fila: {e}")
                                        pass

                    # Si no encuentra en tabla, buscar valores 3.XXX
                    print("No se encontró en tabla, buscando por patrón...")
                    valores = re.findall(r'3\.\d{3}', html)

                    if valores and len(valores) >= 2:
                        # Tomar los primeros dos valores únicos encontrados
                        valores_unicos = []
                        for v in valores:
                            if v not in valores_unicos:
                                valores_unicos.append(v)
                            if len(valores_unicos) == 2:
                                break

                        compra = float(valores_unicos[0])
                        venta = float(valores_unicos[1]) if len(valores_unicos) > 1 else compra

                        # Si no se pudo extraer la fecha del HTML, calcularla
                        if not fecha_sbs:
                            fecha_hoy = datetime.now()
                            dia_semana = fecha_hoy.weekday()

                            if dia_semana == 5:  # Sábado
                                fecha_calc = fecha_hoy - timedelta(days=1)  # Viernes
                            elif dia_semana == 6:  # Domingo
                                fecha_calc = fecha_hoy - timedelta(days=2)  # Viernes
                            else:
                                fecha_calc = fecha_hoy - timedelta(days=1)

                            fecha_sbs = fecha_calc.strftime("%d/%m/%Y")

                        print(f"SBS patrón: Compra={compra}, Venta={venta}, Fecha={fecha_sbs}")
                        return {
                            "compra": compra,
                            "venta": venta,
                            "fecha": fecha_sbs,
                            "fuente": "direct"
                        }

            except Exception as e:
                print(f"Error SBS: {e}")

            return {"error": True, "message": "No se pudo obtener de SBS"}
        
        # Ejecutar
        print("=" * 60)
        print("OBTENIENDO TIPO DE CAMBIO CON SCRAPERAPI")
        print("=" * 60)
        
        sunat_data = get_sunat_scraperapi()
        sbs_data = get_sbs_scraperapi()
        
        result = {
            "sunat": sunat_data,
            "sbs": sbs_data,
            "ultima_actualizacion": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        with open('tipo-cambio.json', 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        print("\n" + "=" * 60)
        print("RESULTADO FINAL:")
        print(f"SUNAT: {sunat_data}")
        print(f"SBS: {sbs_data}")
        print("=" * 60)
        ENDOFSCRIPT
        
        python scraper.py
    
    - name: Mostrar JSON
      run: cat tipo-cambio.json
    
    - name: Commit
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add tipo-cambio.json
        git commit -m "TC via ScraperAPI - $(date +'%d/%m/%Y %H:%M')" || echo "No changes"
        git push || echo "Nothing to push"
